<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../favicon.ico">
  
  <title>Account Takeover Prevention Application - DataTorrent Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Account Takeover Prevention Application";
    var mkdocs_page_input_path = "Accounttakeover.md";
    var mkdocs_page_url = "/Accounttakeover/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script type="text/javascript" src="../js/highlight.pack.js" defer></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-44586211-2', 'docs.datatorrent.com');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DataTorrent Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">DataTorrent RTS</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Demos</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../demos/">Running Apps</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../sandbox/">Sandbox</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Cloud Integration</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../aws_emr_manual/">AWS</a>
                </li>
                <li class="">
                    
    <a class="" href="../azure_deployment/">Azure</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Development</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../create/">Creating Applications</a>
                </li>
                <li class="">
                    
    <a class="" href="../beginner/">Beginner's Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../demo_videos/">Videos</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Tutorials</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../tutorials/topnwords/">Top N Words</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../tutorials/salesdemo/">Sales Dimensions</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../apex_development_setup/">Apex Development Setup</a>
                </li>
                <li class="">
                    
    <a class="" href="../configure_IDE/">Generate New Project in IDE</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_development/">Applications</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_packages/">Application Packages</a>
                </li>
                <li class="">
                    
    <a class="" href="../configuration_packages/">Configuration Packages</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Operator Development</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../operator_development/">Guide</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operator_development_ref/">Reference</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Operators</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../library_operators/">Operators List</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/drools_operator/">Drools Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/python_operator/">Python Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/PMML_operator/">PMML Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/tcpinputoperator/">TCP Input Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/eventhubinput/">Event Hub Input Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/eventhuboutput/">Event Hub Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/abstracthttpserver/">Abstract HTTP Server Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/azure_blob/">Azure Blob Storage Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/aoooperator/">Analytics Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/elasticsearch/">Elasticsearch Output Operator</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/block_reader/">Block Reader</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/deduper/">Deduper</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/dimensions_computation/">Dimension Computation</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/file_output/">File Output</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/file_splitter/">File Splitter</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/hdht/">HDHT</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/kafkaInputOperator/">Kafka Input</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../operators/snapshot_server/">Snapshot Server</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../app_data_framework/">App Data Framework</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_api/">REST API</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">App Templates</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/common/import-launch/">Import and Launch App-template</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/common/customize/">Customizing an app-template</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/database-to-database-sync/">Database-to-database-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-line-copy/">HDFS-line-copy</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-part-file-copy/">HDFS-part-file-copy</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/hdfs-to-hdfs-filter-transform/">HDFS-to-HDFS-filter-transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-cassandra-filter-transform/">Kafka-to-Cassandra-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-database-sync/">Kafka-to-Database-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-hdfs-filter-transform/">Kafka-to-HDFS-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kafka-to-kafka-filter-transform/">Kafka-to-Kafka-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kinesis-to-redshift/">Kinesis-to-Redshift</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/kinesis-to-s3/">Kinesis-to-S3</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/s3-to-hdfs-sync/">S3-to-HDFS-sync</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/0.10.0/s3-to-redshift/">S3-to-HDFS-Filter-Transform</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/database-to-hdfs/">Database dump to HDFS App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/database-to-database-sync/">Database to Database Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-sync/">HDFS Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-line-copy/">HDFS to HDFS Line Copy App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-to-kafka-sync/">HDFS to Kafka Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/hdfs-to-s3-sync/">HDFS to S3 Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-database-sync/">Kafka to Database Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-hdfs-filter/">Kafka to HDFS Filter App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kafka-to-hdfs-sync/">Kafka to HDFS Sync App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/kinesis-to-s3/">Kinesis to S3 App</a>
                </li>
                <li class="">
                    
    <a class="" href="../app-templates/s3-to-hdfs-sync/">S3 to HDFS Sync App</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Applications</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../omni_channel_fraud_app/">Omni Channel Fraud Prevention Application</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Account Takeover Prevention Application</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#quick-launch-account-takeover-application">Quick Launch Account Takeover Application</a></li>
    

    <li class="toctree-l3"><a href="#workflow">Workflow</a></li>
    

    <li class="toctree-l3"><a href="#setting-the-application">Setting the Application</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#prerequisites">Prerequisites</a></li>
        
            <li><a class="toctree-l4" href="#operators-modules">Operators / Modules</a></li>
        
            <li><a class="toctree-l4" href="#configuring-rules">Configuring Rules</a></li>
        
            <li><a class="toctree-l4" href="#configuring-properties">Configuring Properties</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#scaling-the-application">Scaling the Application</a></li>
    

    <li class="toctree-l3"><a href="#launching-the-application">Launching the Application</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#generate-sample-input">Generate Sample Input</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#storing-and-replaying-data">Storing and Replaying Data</a></li>
    

    <li class="toctree-l3"><a href="#syncing-with-the-application-backplane">Syncing with the Application Backplane</a></li>
    

    <li class="toctree-l3"><a href="#dashboards">Dashboards</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#real-time-account-take-over-analysis">Real-time Account Take-over Analysis</a></li>
        
            <li><a class="toctree-l4" href="#real-time-account-take-over-operations">Real-time Account Take-over Operations</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../quickstartlaunchfpa/">Quick Start Guide - Omni-Channel Fraud Prevention Application</a>
                </li>
                <li class="">
                    
    <a class="" href="../quickstartlaunchato/">Quick Start Guide - Account Takeover Prevention Application</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Services</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cep_workbench/">CEP Workbench</a>
                </li>
                <li class="">
                    
    <a class="" href="../oas_dashboards/">OAS Dashboards</a>
                </li>
                <li class="">
                    
    <a class="" href="../oas/">Online Analytics Service (OAS)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Platform</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../rts/">RTS</a>
                </li>
                <li class="">
                    
    <a class="" href="../application_configurations/">Application Configurations</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtmanage/">dtManage</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtdashboard/">dtDashboard</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway/">dtGateway</a>
                </li>
                <li class="">
                    
    <a class="" href="../services/">Services</a>
                </li>
                <li class="">
                    
    <a class="" href="../jar_artifacts/">JAR Artifacts</a>
                </li>
                <li class="">
                    
    <a class="" href="../apex/">Apache Apex</a>
                </li>
                <li class="">
                    
    <a class="" href="../apex_malhar/">Apache Apex-Malhar</a>
                </li>
                <li class="">
                    
    <a class="" href="../appbackplane/">Application Backplane</a>
                </li>
                <li class="">
                    
    <a class="" href="../storeandreplay/">Store and Replay Feature</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Deployment and Operations</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Licensing/">Licensing</a>
                </li>
                <li class="">
                    
    <a class="" href="../installation/">Installation</a>
                </li>
                <li class="">
                    
    <a class="" href="../configuration/">Configuration</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_security/">Security</a>
                </li>
                <li class="">
                    
    <a class="" href="../dtgateway_systemalerts/">System Alerts</a>
                </li>
                <li class="">
                    
    <a class="" href="../apexcli/">Apex CLI</a>
                </li>
                <li class="">
                    
    <a class="" href="../troubleshooting/">Troubleshooting</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../release_notes/">Release Notes</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../glossary/">Glossary</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../additional_docs/">Resources</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DataTorrent Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Applications &raquo;</li>
        
      
    
    <li>Account Takeover Prevention Application</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Account Takeover (ATO) application is a pre-built application that can be used to detect and prevent attempts of account takeover in various industries such as finance, telecom, and subscription-based offerings. ATO application is designed to ingest, transform, analyze incoming data, and provide account takeover alerts in real-time before such an activity occurs. It also provides visualizations of incoming as well as processed data.</p>
<p>Using this application, you can process, enrich, analyze and act in real-time upon multiple streams of account event information which can prevent account-take over and fraud.</p>
<p>ATO application is built over DataTorrent RTS platform and can be run on commodity hardware. The platform provides real time insights and a fault tolerant and scalable method for processing data. The application can be further customized by writing rules as per your business needs and implementing custom data types.</p>
<p>In addition, ATO has the capability to store and replay the incoming data from Kafka input operators and then replay the stored data with a different set of rules to visualize the outcome.  You can also integrate the application backplane, to share the fraud outcomes of ATO with other fraud detection-based applications and thereby reducing the chance of fraud. 
The following services come preinstalled with an ATO application. You can run these services for analytics, visualizing the analytic outcomes, and for creating customized rules.</p>
<ul>
<li><a href="../oas/">Online Analytics Service</a></li>
<li><a href="../oas_dashboards/">OAS Dashboards</a></li>
<li><a href="../cep_workbench/">CEP Workbench</a></li>
</ul>
<p>ATO application is available with <strong>DT Premium license</strong>.</p>
<h2 id="quick-launch-account-takeover-application">Quick Launch Account Takeover Application</h2>
<p>For details about quickly launching the application, refer to <a href="../quickstartlaunchato/">Quick Start Guide - Account Takeover Prevention Application</a>.  </p>
<h1 id="workflow">Workflow</h1>
<p>The following image depicts the workflow in the ATO application:
<img alt="" src="../images/applications/ATOworkflow.png" /></p>
<h1 id="setting-the-application">Setting the Application</h1>
<p>Before you run the  <strong>ATO</strong>  application, you must ensure to fulfill the prerequisites and to configure the operators in the DAG.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>The following should be installed on the cluster before setting up the application:</p>
<table>
<thead>
<tr>
<th><strong>Product</strong></th>
<th><strong>Version</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Apache Hadoop</td>
<td>2.6.0 and Above</td>
<td>Apache Hadoop is an open-source software framework that is used for distributed storage and processing of dataset of big data using the MapReduce programming model.</td>
</tr>
<tr>
<td>DataTorrent RTS</td>
<td>3.10.0</td>
<td>DataTorrent RTS, which is built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, data ingestion, and distribution features.</td>
</tr>
<tr>
<td>Apache Kafka</td>
<td>0.9</td>
<td>Apache Kafka is an open-source stream processing platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.</td>
</tr>
</tbody>
</table>
<h2 id="operators-modules">Operators / Modules</h2>
<p>The following operators/modules are included for the ATO application.</p>
<table>
<thead>
<tr>
<th><strong>Operator / Module</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>User Activity Receiver</td>
<td>This Kafka input operator receives current user activity from Syslogs or any other logs. It forwards these logs to downstream operator.</td>
</tr>
<tr>
<td>User Activity Parser</td>
<td>This JSON parser operator parses the incoming messages and converts them into plain java objects hereafter referred as tuple for further processing.</td>
</tr>
<tr>
<td>User Profile Enricher</td>
<td>This operator gets the relevant JAVA applicable user details corresponding to a unique ID and enriches the tuple. User details can be extracted from JDBC database store or json file on HDFS.  You can configure the operator based on the enrichment data source you choose.Using this operator is optional in an ATO application.</td>
</tr>
<tr>
<td>Geo Data Enricher</td>
<td>The application identifies the geolocation of the transaction by performing a lookup of the transaction IP against the external database like Maxmind database.  Using this operator is optional in an ATO application.</td>
</tr>
<tr>
<td>Rules Executor</td>
<td>This operator is the Drools Operator. It applies the pre-defined rules to the incoming tuples and takes a suitable action depending on the outcome of the rules applied to a tuple.</td>
</tr>
<tr>
<td>Output Module</td>
<td>Output module consists of two operators:<strong>Avro Serializer</strong> serializes the output of the Rules Executor (Drools operator) to send to Kafka Output operator. <strong>Kafka Output Operator</strong> sends these events to the specified Kafka topic for consumption by other applications. This publishes the information which can be consumed by Omni-Channel Fraud prevention application.</td>
</tr>
<tr>
<td>HDFS Output Operator</td>
<td>This output operator writes messages coming from the Rules Executor to the specified HDFS file path. Using this operator is optional in an ATO application.</td>
</tr>
<tr>
<td>OAS Operator</td>
<td>This operator writes messages to a Kafka topic that are consumed by Online Analytics Service (OAS).</td>
</tr>
</tbody>
</table>
<h2 id="configuring-rules">Configuring Rules</h2>
<p>The application package contains sample rules. However, you can add rules based on your business requirements. These can be configured in Drools supported formats such as . <strong>drl</strong> , <strong>xls etc</strong>. Refer  <a href="https://docs.jboss.org/drools/release/7.2.0.Final/drools-docs/html_single/#drools.AuthoringAssets">Authoring Rule Assets</a> in Drools documentation.</p>
<p>For the Rules Executor, you can configure the rules either from the CEP Workbench or from HDFS.</p>
<h3 id="cep-work-bench">CEP Work Bench</h3>
<p>To configure rules from CEP Workbench,  you must configure the kieBaseName and kieSessionName properties to the application configuration where you are implementing the rules that you have created using CEP Workbench. Refer to <a href="../cep_workbench/">CEP Workbench</a></p>
<h3 id="hdfs">HDFS</h3>
<p>To configure rules from HDFS, do the following:</p>
<ol>
<li>Create the rules file in one of the format that is supported by Drools and save the <strong>output rule</strong> file onto your local machine.</li>
<li>Copy this rule file into the HDFS folder.</li>
<li>In the Droolsoperator, configure the folder path in the following operator property, to point to HDFS folder containing rules.</li>
</ol>
<table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>rulesDir</td>
<td>The path to HDFS from where you can load the rules. If this path is set to null, then operator loads the rules from the classpath.</td>
</tr>
</tbody>
</table>
<ol>
<li>Restart the application after updating the rules.</li>
</ol>
<p><strong>Note:</strong> When the folder path is not provided to the Drools operator, the packaged rules are uploaded by default.</p>
<h2 id="configuring-properties">Configuring Properties</h2>
<p>The properties for the following items must be set for running ATO application:</p>
<ul>
<li><a href="#kafka">Kafka</a></li>
<li><a href="#parser">Parser</a></li>
<li><a href="#userprofile">User Profile Enricher</a></li>
<li><a href="#geodata">Geo Data Enricher</a></li>
<li><a href="#rulesexecutor">Rules Executor</a></li>
<li><a href="#avro">Avro Serializer</a></li>
<li><a href="#hdfsoutputoperator">HDFS Output Operator</a></li>
<li><a href="#aoo">AOO Operator</a></li>
</ul>
<h3 id="kafka"><a name="kafka"></a>Kafka</h3>
<p><strong>User Activity Receiver</strong> operator and <strong>Kafka</strong><strong> Output</strong> operator are the respective entry and exit points of the application. These operators read from the Kafka topics and write to the Kafka topics. Therefore, you must ensure that the kafka setup in the system is up and running.</p>
<p>Configure the kafka setup details in the application properties file.  The following required properties must be configured:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
<th><strong>Required</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>kafkaBrokerList</td>
<td>Comma separated list of kafka-brokers</td>
<td>String</td>
<td>node1.company.com:9098, node2.company.com:9098, node3.company.com:9098</td>
<td>Yes</td>
</tr>
<tr>
<td>UserActivityReceiverTopic</td>
<td>Topics to read from Kafka</td>
<td>String</td>
<td>transactions</td>
<td>Yes</td>
</tr>
<tr>
<td>ProcessedTransactionsOutputTopic</td>
<td>Topics to write processed transactions to kafka</td>
<td>String</td>
<td>processed-transactions</td>
<td>Yes</td>
</tr>
<tr>
<td>initialOffset</td>
<td>Initial offset to read from Kafka</td>
<td>String</td>
<td>EARLIESTLATESTAPPLICATION_OR_EARLIESTAPPLICATION_OR_LATEST</td>
<td></td>
</tr>
<tr>
<td>key.serializer</td>
<td>Serializer class</td>
<td>String</td>
<td>org.apache.kafka.common.serialization.StringSerializer</td>
<td></td>
</tr>
<tr>
<td>value.serializer</td>
<td>Serializer class</td>
<td>String</td>
<td>org.apache.kafka.common.serialization.StringSerializer</td>
<td></td>
</tr>
<tr>
<td>dt.operator.TransactionDataPublisher.prop.properties(key.serializer)</td>
<td>Serializer class</td>
<td>String</td>
<td>org.apache.kafka.common.serialization.StringSerializer</td>
<td></td>
</tr>
<tr>
<td>dt.operator.TransactionDataPublisher.prop.properties(value.serializer)</td>
<td>Serializer class</td>
<td>String</td>
<td>org.apache.kafka.common.serialization.StringSerializer</td>
<td></td>
</tr>
<tr>
<td>archivePath</td>
<td>Path of archive directory where you can store data for replaying with different rules. [Refer <a href="../storeandreplay/">Store and Replay</a></td>
<td>String</td>
<td></td>
<td></td>
</tr>
<tr>
<td>archiveType</td>
<td>Archive information</td>
<td>Enum</td>
<td>ARCHIVE_TYPE_KAFKA</td>
<td></td>
</tr>
<tr>
<td>enableArchive</td>
<td>to enable / disable archiving for replaying data.</td>
<td>Boolean</td>
<td></td>
<td></td>
</tr>
<tr>
<td>enableReplay</td>
<td>Value to enable or disable replay. enableReplay is mutually exclusive with enableArchive, both can be false.</td>
<td>Boolean</td>
<td></td>
<td></td>
</tr>
<tr>
<td>whenReplaySetT0</td>
<td>When enable Replay is true, this can be set. Set the start time from when to replay. Format is <strong>yyyy-MM-dd&#39;T&#39;HH:mm:ss</strong></td>
<td>String</td>
<td>2017-09-17T01:01:01</td>
<td></td>
</tr>
<tr>
<td>whenReplaySetT1</td>
<td>When enable Replay is true, this can be set. Set the start time to replay. Format is <strong>yyyy-MM-dd&#39;T&#39;HH:mm:ss</strong></td>
<td>String</td>
<td>2017-09-17T03:01:01</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="parser"><a name="parser"></a>Parser</h3>
<p>Parser parses JSON input from kafka and generates plain JAVA object for further processing.</p>
<p>Configure the JAVA class of the plain JAVA object to be generated by parser. The following properties must be set for the parser:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TUPLE_CLASS schema attribute</strong></td>
<td>pojo class name of object to be generated by parser</td>
<td>String</td>
<td>com.datatorrent.ato.schema.UserActivity</td>
</tr>
</tbody>
</table>
<h3 id="user-profile-enricher"><a name="userprofile"></a>User Profile Enricher</h3>
<p>Missing fields from your incoming records can be enriched by referring to your lookup data in enrichment phase. By default, the configuration for enrichment is stored in <strong>enrichments.json</strong> that is bundled in the application package. You can also write your own configuration in a file and store that file on HDFS. You can configure existing enrichments or add / remove as per your business needs. The enrichment properties file path should be provided in the <strong>properties.xml</strong> file.</p>
<p>Following properties should be set for configuration file path:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>dt.atoapp.enrichments.configFilePath</td>
<td>Path of configuration file for enrichments</td>
<td>string</td>
<td>enrichments.json</td>
</tr>
</tbody>
</table>
<h3 id="geo-data-enricher"><a name="geodata"></a>Geo Data Enricher</h3>
<p>GeoData Enrichment operator refers to the <strong>maxmind</strong> database that fetch geo information when you provide the IP address of the transaction location. To run the GeoData Enrichment operator, you must copy the maxmind city database (GeoLite2 City) to HDFS. You can remove this enrichment as well as update properties (e.g. maxmind db path) by configuring enrichment properties. &lt;Please mention again as per previous para how to write and set enrichments.json&gt;</p>
<p><strong>Note:</strong> Extract <strong>GeoLite2-City.mmdb</strong> file to HDFS. Do not copy the ZIP file directly.</p>
<p>Following properties should be set for the <strong>User Profile Enricher</strong> operator as well as the <strong>Geo Data Enricher</strong> operator in the <strong>enrichments.json</strong> file.</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>storeType</td>
<td>Type of data storage</td>
<td>string</td>
<td>json_file, geo_mmdb, jdbc</td>
</tr>
<tr>
<td>storeFields</td>
<td>Json array of Name of fields in input objectsa</td>
<td>string</td>
<td>[{ &quot;name&quot; : &quot;userId&quot;, &quot;type&quot; : &quot;string&quot; },  { &quot;name&quot;: &quot;customerType&quot;, &quot;type&quot; : &quot;string&quot; }]</td>
</tr>
<tr>
<td>inputType</td>
<td>Type of input object</td>
<td>string</td>
<td>com.datatorrent.ato.schema.UserActivity</td>
</tr>
<tr>
<td>outputType</td>
<td>Type of output object</td>
<td>string</td>
<td>com.datatorrent.ato.schema.UserActivity</td>
</tr>
<tr>
<td>reuseObject</td>
<td>Specify if object can be reused</td>
<td>boolean</td>
<td>true</td>
</tr>
<tr>
<td>file</td>
<td>Path of the user data file</td>
<td>string</td>
<td>ato_lookupdata/customers.json</td>
</tr>
<tr>
<td>refreshInterval</td>
<td>Time interval after which cache should be refreshed</td>
<td>integer</td>
<td>5000</td>
</tr>
<tr>
<td>lookupFields</td>
<td>Main field / key based on which the user data is queried.</td>
<td>JSON</td>
<td>userId</td>
</tr>
<tr>
<td>includeFields</td>
<td>comma seperated Mapping of fields from user data to the fields in JAVA object.</td>
<td>JSON</td>
<td>&quot;customer.userId&quot;:&quot;userId&quot;,&quot;customer.customerType&quot;:&quot;customerType&quot;</td>
</tr>
</tbody>
</table>
<h4 id="example-of-enrichmentjson-file">Example of enrichment.json file</h4>
<p>Following is an example of the <strong>enrichment.json</strong> file. You can refer to this example to create the <strong>enrichment.json</strong> file.</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;UserProfileEnricher&quot;,
    &quot;storeType&quot; : &quot;json_file&quot;,
    &quot;storeFields&quot; : [
    { &quot;name&quot; : &quot;userId&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot;: &quot;customerType&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;customerAvgSpending&quot;, &quot;type&quot; : &quot;double&quot; },
    { &quot;name&quot; : &quot;customerRiskScore&quot;, &quot;type&quot; : &quot;double&quot; },
    { &quot;name&quot; : &quot;custGender&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot;: &quot;custMaritalStatus&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custIncomeLevel&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custStreet1&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custStreet2&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custCity&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custState&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custCountry&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custPoBox&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custPostalCode&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;custPostalCodeType&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot; : &quot;lat&quot;, &quot;type&quot; : &quot;double&quot; },
    { &quot;name&quot; : &quot;lon&quot;, &quot;type&quot; : &quot;double&quot; }
    ],
    &quot;inputType&quot; : &quot;com.datatorrent.ato.schema.UserActivity&quot;,
    &quot;outputType&quot; : &quot;com.datatorrent.ato.schema.UserActivity&quot;,
    &quot;reuseObject&quot; : true,
    &quot;properties&quot;: {
    &quot;file&quot; : &quot;ato_lookupdata/customers.json&quot;,
    &quot;refreshInterval&quot; : 5000
    },
    &quot;lookupFields&quot; : {
    &quot;userId&quot; : &quot;userId&quot;
    },
    &quot;includeFields&quot; : {
    &quot;customer.userId&quot;:&quot;userId&quot;,
    &quot;customer.customerType&quot;:&quot;customerType&quot;,
    &quot;customer.customerAvgSpending&quot;:&quot;customerAvgSpending&quot;,
    &quot;customer.customerRiskScore&quot;:&quot;customerRiskScore&quot;,
    &quot;customer.custGender&quot;:&quot;custGender&quot;,
    &quot;customer.custMaritalStatus&quot;:&quot;custMaritalStatus&quot;,
    &quot;customer.custIncomeLevel&quot;:&quot;custIncomeLevel&quot;,
    &quot;customer.custStreet1&quot;:&quot;custStreet1&quot;,
    &quot;customer.custStreet2&quot;:&quot;custStreet2&quot;,
    &quot;customer.custCity&quot;:&quot;custCity&quot;,
    &quot;customer.custState&quot;:&quot;custState&quot;,
    &quot;customer.custCountry&quot;:&quot;custCountry&quot;,
    &quot;customer.custPoBox&quot;:&quot;custPoBox&quot;,
    &quot;customer.custPostalCode&quot;:&quot;custPostalCode&quot;,
    &quot;customer.custPostalCodeType&quot;: &quot;custPostalCodeType&quot;
    }
  },
  {
    &quot;name&quot;: &quot;GeoDataEnricher&quot;,
    &quot;passThroughOnError&quot; : true,
    &quot;storeType&quot;: &quot;geo_mmdb&quot;,
    &quot;storeFields&quot;: [
    { &quot;name&quot; : &quot;IP&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot;: &quot;CITY&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot; : &quot;SUBDIVISION_ISO&quot;, &quot;type&quot; : &quot;string&quot; },
    { &quot;name&quot;: &quot;ZIPCODE&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot;: &quot;COUNTRY_ISO&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot; : &quot;LATITUDE&quot;, &quot;type&quot; : &quot;double&quot; },
    { &quot;name&quot; : &quot;LONGITUDE&quot;, &quot;type&quot; : &quot;double&quot; }
    ],
    &quot;inputType&quot;: &quot;com.datatorrent.ato.schema.UserActivity&quot;,
    &quot;outputType&quot;: &quot;com.datatorrent.ato.schema.UserActivity&quot;,
    &quot;reuseObject&quot;: true,
    &quot;properties&quot;: {
    &quot;dbpath&quot;: &quot;city.mmdb&quot;,
    &quot;refreshInterval&quot;: 5000
    },
    &quot;lookupFields&quot;: {
    &quot;IP&quot;: &quot;deviceIP&quot;
    },
    &quot;includeFields&quot;: {
    &quot;geoIp.city&quot; : &quot;CITY&quot;,
    &quot;geoIp.state&quot; : &quot;SUBDIVISION_ISO&quot;,
    &quot;geoIp.zipcode&quot; : &quot;ZIPCODE&quot;,
    &quot;geoIp.country&quot; : &quot;COUNTRY_ISO&quot;,
    &quot;geoIp.latitude&quot; : &quot;LATITUDE&quot;,
    &quot;geoIp.longitude&quot; : &quot;LONGITUDE&quot;
    }
  }
]
</code></pre>

<h3 id="rules-executor"><a name="rulesexecutor"></a>Rules Executor</h3>
<p>The Rules Executor that is the Drools operator provides a method to load rules from:</p>
<ul>
<li>CEP Workbench</li>
<li>HDFS</li>
</ul>
<p>If rules are loaded from files on HDFS, you must configure the following property:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>rulesDir</td>
<td>Path to HDFS from where to load the rules. If this path is set to null, then the operator loads the rules from the classpath.</td>
<td>string</td>
</tr>
</tbody>
</table>
<p>If rules are to be loaded from CEP Workbench, you must specify following properties. Also Refer <a href="../cep_workbench/">CEP Workbench</a></p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>kieSessionName</td>
<td>If rules are to be loaded from application classpath, then specify the name of the session to use. This is created using CEP Worknbench.</td>
<td>string</td>
<td>UserActivity-rules-session</td>
</tr>
<tr>
<td>kiebaseName</td>
<td>If rules are to be loaded from application classpath, then specify the name of the kie base (rule) to use . This is created using CEP Workbench.</td>
<td>string</td>
<td>ato-rules</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> If rules are to be loaded from application classpath, the knowledge jar (KJAR) should be in the classpath.  Refer to <a href="../application_configurations/">Application Configurations</a></p>
<h3 id="avro-serializer"><a name="avro"></a>Avro Serializer</h3>
<p>The following properties should be set for the Avro Serializer operator that is part of the Output module:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>dt.atoapp.schemaName</td>
<td>Set schema for the data.</td>
<td>string</td>
<td>UserActivity</td>
</tr>
<tr>
<td>topic</td>
<td>Set the kafka topic name</td>
<td>string</td>
<td>ATO_analyseddata</td>
</tr>
</tbody>
</table>
<h3 id="hdfs-output-operator"><a name="hdfsoutputoperator"></a>HDFS Output Operator</h3>
<p>There are two output operators to write to HDFS:</p>
<ul>
<li><strong>ProcessedActivityDataWriter</strong>
This operator writes all the transactions processed by the application to HDFS.</li>
<li><strong>FlaggedActivityFileWriter</strong>
This operator writes only the fraud user activities to the HDFS.</li>
</ul>
<p>For details of other properties of FileOutput Operator, please refer the <a href="http://docs.datatorrent.com/operators/file_output/">documentation</a>.</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>dt.atoapp.enableOutputOperators</td>
<td>This flag should be set to true if messages are to be written to HDFS</td>
<td>boolean</td>
<td></td>
</tr>
<tr>
<td>filePath</td>
<td>Path of the directory where the output files must be created.</td>
<td>string</td>
<td>/user/dtuser/processeddata</td>
</tr>
<tr>
<td>outFileName</td>
<td>Name of the file on HDFS in which the messages should be written.</td>
<td>string</td>
<td>ProcessedUserActivity</td>
</tr>
</tbody>
</table>
<h3 id="aoo-operator"><a name="aoo"></a>AOO Operator</h3>
<p>This operator writes messages to a Kafka topic that are consumed by the OAS (Online Analytics Service). The following properties should be set:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>schema</td>
<td>Schema / metadata of the data to be sent to OAS.By default we package &quot;analyticsschema.json&quot; schema to change schema copy your schema file to hdfs and configure, &quot;dt.atoapp.analytics.resourceFileName&quot; with your schema file path.</td>
<td>string</td>
<td>analyticsschema.json</td>
</tr>
<tr>
<td>serializerClass</td>
<td>Provides information about serializing incoming messages in the form of JAVA objects to send to Kafka</td>
<td>string</td>
<td>com.datatorrent.cep.common.ToStringAnalyticsPojoSerializer</td>
</tr>
<tr>
<td>disablePartialWindowCheck</td>
<td>Set whether to disable partition window check or not.   <strong>Note</strong>: By disabling the partition window check duplicate data can be sent to Kafka thereby overriding exactly once guarantees.</td>
<td>boolean</td>
<td>&lt;example&gt;</td>
</tr>
</tbody>
</table>
<h1 id="scaling-the-application">Scaling the Application</h1>
<p>To handle higher data loads, you can add more partitions of the processing units i.e. operators.</p>
<p>Update the following properties as per your input load. The following properties must be set for scaling the application:</p>
<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Description</strong></th>
<th><strong>Type</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>UserActivityReceiver.initialPartitionCount</td>
<td>Partition count of Kafka data receiver.</td>
<td>Integer</td>
<td>1</td>
</tr>
<tr>
<td>RulesExecutor.attr.PARTITIONER</td>
<td>Partition count of Rule execution operator.</td>
<td>Integer</td>
<td>1</td>
</tr>
<tr>
<td>FlaggedActivityFileWriter.partitionedFileNameformat</td>
<td>File name format for transaction writer partition.</td>
<td>String</td>
<td>%s-%04d</td>
</tr>
<tr>
<td>ProcessedActivityDataWriter.partitionedFileNameformat</td>
<td>File name format for fraud writer partition.</td>
<td>String</td>
<td>%s-%04d</td>
</tr>
<tr>
<td>RulesExecutor.port.factsInput.attr.STREAM_CODEC</td>
<td>Ensure that all related tuples should go to same partition so that tuples can be co-related across time to do complex event processing.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Set STREAM_CODEC property of factsInput port of RulesExecutor to make sure related tuples go to same partition</td>
<td>String</td>
<td>com.datatorrent.cep.common.codec.ConfigurableStreamCodec:userId</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="launching-the-application">Launching the Application</h1>
<p>The Account Takeover application can be launched from the DataTorrent RTS interface.</p>
<p>To launch the application, do the following:</p>
<ol>
<li>Go to the <strong>Develop</strong> page and upload the application package.</li>
<li>Specify the configuration as per the <strong>Application Configuration</strong> section. <a href="../application_configurations/">Application Configurations</a></li>
<li>Launch the application. 
During the launch process, you can name the configuration and save it for future references. After you launch, you can track the details of the processed data in the application from the <strong>Monitor</strong> tab.</li>
</ol>
<h2 id="generate-sample-input">Generate Sample Input</h2>
<p>For a test run, you may want to generate sample data.</p>
<p>To generate sample data, do the following:</p>
<ol>
<li>Run the <strong>dt-ato-datagen-1.4.0.apa</strong> application from the DataTorrent RTS interface.</li>
<li>Specify kafka server details and topic name which must be the same as configured for User Activity Receiver.</li>
</ol>
<h1 id="storing-and-replaying-data">Storing and Replaying Data</h1>
<p>A service is available for the ATO application to store and replay events. You can record and replay data from a point in time to evaluate the effectiveness of builds, models, rules, and scenarios before they are put into production. Refer <a href="../storeandreplay/">Store and Replay</a></p>
<h1 id="syncing-with-the-application-backplane">Syncing with the Application Backplane</h1>
<p>Application Backplane enables multiple applications to be integrated to share insights and actions. ATO application is combined with Fraud Prevention application in real-time. Both these applications remain independent and yet benefit from a network-effect. For more details refer to <a href="../appbackplane/">Application Backplane</a></p>
<h1 id="dashboards">Dashboards</h1>
<p>The ATO Prevention application includes the following dashboards:</p>
<ul>
<li>Real-time Account Take-over Analysis</li>
<li>Real-time Account Take-over Operations</li>
</ul>
<h2 id="real-time-account-take-over-analysis">Real-time Account Take-over Analysis</h2>
<p><img alt="" src="../images/applications/ATO-analysis.png" />
The analysis dashboard provides a granular set of metrics for analysis of the account activity in ATO application. The following metrics are shown in this dashboard:</p>
<ul>
<li>Account Takeover Fraud Prevention Analysis</li>
<li>Account Hacking Breakdown by Channel</li>
<li>Total ATO Frauds</li>
<li>ATO Fraud Breakdown by Channel</li>
<li>Fraud Instances by Device</li>
<li>ATO Frauds</li>
<li>Frauds by EventType</li>
<li>Fraud Rule Matches</li>
<li>ATO Fraud in the USA (easily customizable for other countries)</li>
<li>Login failures by device type</li>
</ul>
<h2 id="real-time-account-take-over-operations">Real-time Account Take-over Operations</h2>
<p><img alt="" src="../images/applications/ATO-operations.png" /></p>
<p>The operations dashboard provides an operational overview of the application.  The following operational metrics can be viewed from this dashboard:</p>
<ul>
<li>Application Latency</li>
<li>Rule Latency</li>
<li>Rule Executor Free Memory</li>
<li>Total Number of Process Failures</li>
<li>Fraud Breakdown by Channel</li>
<li>Event Throughput</li>
<li>Top Rules</li>
<li>Events by Device Type</li>
</ul>
<p>You can use the OAS dashboards service that is specific to ATO application to visualize various metrics over the dashboards. <a href="../oas_dashboards/">OAS Dashboards</a></p>
<p>For importing and viewing the dashboards, refer to <a href="../dtdashboard/">dtDashboard</a></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../quickstartlaunchfpa/" class="btn btn-neutral float-right" title="Quick Start Guide - Omni-Channel Fraud Prevention Application">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../omni_channel_fraud_app/" class="btn btn-neutral" title="Omni Channel Fraud Prevention Application"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  <div class="footer">Copyright &#169; 2018 DataTorrent</div>
  <div class="footer">Apache, Hadoop, Apex, Yarn, HDFS and the Hadoop elephant and Apache project logos are either registered trademarks or trademarks of the Apache Software Foundation in the United States or other countries.</div>
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
  <div class="version-select">
    <div class="version-select__bar">
      v: <span class="version-select__bar-version"></span>
      <i class="fa fa-caret-down"></i>
    </div>
    <div class="version-select__menu">
      <dl class="version-select__menu-versions">
        <dt>Versions</dt>
        <dd><a class="version-select__menu-versions-latest version" href="#">latest</a></dd>
        <!-- append versions here -->
      </dl>
    </div>
  </div>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/require.js" defer></script>
      <script src="../search/search.js" defer></script>

</body>
</html>
